import argparse
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time

class NovelDownloader:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        })
        self.visited_urls = set()
        self.current_chapter = None
        self.chapter_content = []
        self.chapter_count = 0

    def clean_chapter_title(self, title):
        """æ¸…ç†ç« èŠ‚æ ‡é¢˜ä¸­çš„åˆ†é¡µä¿¡æ¯"""
        # å»é™¤æ‹¬å·å†…å®¹ï¼ˆå¦‚"ç¬¬ä¸€ç«  (2/3)" â†’ "ç¬¬ä¸€ç« "ï¼‰
        title = re.sub(r"\(.*?\)|ï¼ˆ.*?ï¼‰|ã€.*?ã€‘", "", title)
        # å»é™¤é¡µç æ ‡è®°ï¼ˆå¦‚"ç¬¬ä¸€ç«  - ç¬¬2é¡µ" â†’ "ç¬¬ä¸€ç« "ï¼‰
        title = re.sub(r"[-â€”]\s*ç¬¬?[0-9]+[é¡µç« ç¯€]?", "", title)
        # å»é™¤é¦–å°¾ç©ºç™½
        return title.strip()

    def extract_chapter_title(self, soup):
        """æå–ç« èŠ‚æ ‡é¢˜"""
        # ä¼˜å…ˆä»<h1>æ ‡ç­¾è·å–
        h1 = soup.find("h1")
        if h1 and h1.get_text(strip=True):
            title = h1.get_text(strip=True)
            return self.clean_chapter_title(title)
        
        # æ¬¡é€‰ä»<title>è·å–
        if soup.title:
            title = soup.title.get_text(strip=True)
            # å»é™¤ç½‘ç«™åéƒ¨åˆ†ï¼ˆå¦‚"ç¬¬ä¸€ç«  - æŸæŸå°è¯´ç½‘"ï¼‰
            title = re.split(r"[-|_|â€”]", title)[0].strip()
            return self.clean_chapter_title(title)
        
        return "æœªå‘½åç« èŠ‚"

    def extract_main_content(self, soup):
        """æå–æ­£æ–‡å†…å®¹"""
        # å¸¸è§æ­£æ–‡é€‰æ‹©å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰
        selectors = [
            {"name": "div", "id": "content"},
            {"name": "div", "class": "content"},
            {"name": "div", "id": "chapter-content"},
            {"name": "div", "class": "chapter-content"},
            {"name": "article"},
            {"name": "div", "class": re.compile(r"read|text|article")},
            {"name": "div", "id": re.compile(r"content|chapter")},
        ]
        
        for selector in selectors:
            content = soup.find(**selector)
            if content and len(content.get_text(strip=True)) > 100:
                # æ¸…ç†ä¸éœ€è¦çš„å…ƒç´ 
                for elem in content.find_all(["script", "style", "div.ad", "ins", "iframe"]):
                    elem.decompose()
                return content.get_text("\n", strip=True)
        
        return None

    def extract_next_page(self, soup, current_url):
        """æå–ä¸‹ä¸€é¡µé“¾æ¥"""
        # 1. æŸ¥æ‰¾åŒ…å«ä¸‹ä¸€é¡µå…³é”®è¯çš„é“¾æ¥
        next_keywords = ["ä¸‹ä¸€é¡µ", "ä¸‹ä¸€ç« ", "ä¸‹ä¸€èŠ‚", "ä¸‹ä¸€é ", "Next", ">", "â€º"]
        for keyword in next_keywords:
            next_link = soup.find("a", string=re.compile(fr"^{keyword}$", flags=re.IGNORECASE))
            if next_link and next_link.get("href"):
                return next_link["href"]
        
        # 2. æŸ¥æ‰¾å¸¸è§åˆ†é¡µclassçš„é“¾æ¥
        for class_name in ["next", "next-page", "pagination-next"]:
            next_link = soup.find("a", class_=class_name)
            if next_link and next_link.get("href"):
                return next_link["href"]
        
        # 3. åˆ†æURLè§„å¾‹ï¼ˆå¦‚/page/2 â†’ /page/3ï¼‰
        if "page" in current_url.lower():
            match = re.search(r"(page|p)=(\d+)", current_url, re.IGNORECASE)
            if match:
                next_num = int(match.group(2)) + 1
                return current_url.replace(
                    f"{match.group(1)}={match.group(2)}", 
                    f"{match.group(1)}={next_num}"
                )
        
        return None

    def process_page(self, url, output_file):
        """å¤„ç†å•ä¸ªé¡µé¢"""
        if url in self.visited_urls:
            return True
        
        self.visited_urls.add(url)
        # print(f"å¤„ç†: {url}")
        
        try:
            # è¯·æ±‚é¡µé¢
            time.sleep(1)  # ç¤¼è²Œæ€§å»¶è¿Ÿ
            response = self.session.get(url, timeout=10)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, "html.parser")
            
            # æå–ä¿¡æ¯
            chapter_title = self.extract_chapter_title(soup)
            content = self.extract_main_content(soup)
            
            if not content:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹: {url}")
                return False
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°ç« èŠ‚
            if chapter_title != self.current_chapter:
                self.save_current_chapter(output_file)
                self.current_chapter = chapter_title
                self.chapter_content = []
                self.chapter_count += 1
            
            # æ·»åŠ å†…å®¹
            self.chapter_content.append(content)
            
            # è·å–ä¸‹ä¸€é¡µ
            next_page = self.extract_next_page(soup, url)
            if next_page and not next_page.startswith(("http://", "https://")):
                next_page = urljoin(url, next_page)
            
            return next_page
        
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {url} - {str(e)}")
            return False

    def save_current_chapter(self, output_file):
        """ä¿å­˜å½“å‰ç« èŠ‚å†…å®¹"""
        if not self.current_chapter or not self.chapter_content:
            return
        
        with open(output_file, "a", encoding="utf-8") as f:
            # å†™å…¥ç« èŠ‚æ ‡é¢˜
            f.write(f"{self.current_chapter}\n")
            # å†™å…¥ç« èŠ‚å†…å®¹
            f.write("".join(self.chapter_content) + "\n")
        
        print(f"âœ… å·²ä¿å­˜ç« èŠ‚: {self.current_chapter}")

    def download_novel(self, start_url, output_file="novel.txt"):
        """ä¸‹è½½å°è¯´å¹¶åˆå¹¶ç« èŠ‚"""
        # æ¸…ç©ºæˆ–åˆ›å»ºè¾“å‡ºæ–‡ä»¶
        with open(output_file, "w", encoding="utf-8") as f:
            f.write("")
        
        current_url = start_url
        while current_url:
            current_url = self.process_page(current_url, output_file)
        
        # ä¿å­˜æœ€åä¸€ç« 
        self.save_current_chapter(output_file)
        print(f"\nğŸ‰ ä¸‹è½½å®Œæˆï¼å…± {self.chapter_count} ç« ï¼Œå·²ä¿å­˜åˆ° {output_file}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='NovelDownloader',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    parser.add_argument(
        '-u', '--url',
        required=True,
        help='èµ·å§‹ç« èŠ‚URLï¼ˆå¿…é¡»ï¼‰',
        metavar='URL'
    )
    parser.add_argument(
        '-o', '--output',
        default='novel.txt',
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„',
        metavar='FILE'
    )
    parser.add_argument(
        '--timeout',
        type=int,
        default=10,
        help='è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰'
    )
    
    return parser.parse_args()

def main():
    # è§£æå‚æ•°
    args = parse_arguments()
    
    # å¼€å§‹ä¸‹è½½
    start_time = time.time()
    print(f"å¼€å§‹ä¸‹è½½: {args.url}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    
    try:
        nd = NovelDownloader()
        nd.download_novel(
            start_url=args.url,
            output_file=args.output
        )
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å·²ä¸‹è½½å†…å®¹...")
    except Exception as e:
        print(f"\nå‘ç”Ÿé”™è¯¯: {str(e)}")
    finally:
        print(f"æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")


if __name__ == "__main__":
    main()